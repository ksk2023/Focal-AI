"""
AI Photography Director - å®Œæ•´ç‰ˆ Streamlit å‰ç«¯åº”ç”¨
æ•´åˆç‰ˆï¼šå®æ—¶æ‘„åƒå¤´ + MediaPipe + å§¿æ€åŒ¹é… + GPTåˆ†æ + è¯­éŸ³åé¦ˆ + è‡ªåŠ¨æ‹ç…§
"""

import streamlit as st
import cv2
import numpy as np
import json
import time
import base64
import httpx
import asyncio
import threading
from datetime import datetime
from typing import Optional, Dict, List
from pathlib import Path
import mediapipe as mp
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode
import av

# å¯¼å…¥å§¿æ€åŒ¹é…æ¨¡å—
from pose_matcher import (
    Landmark,
    calculate_pose_similarity,
    get_feedback_instruction,
    get_detailed_analysis,
    TARGET_POSES,
)

# ==================== é¡µé¢é…ç½® ====================

st.set_page_config(
    page_title="AI æ‹ç…§åŠ©æ‰‹",
    page_icon="ğŸ“¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== åŠ è½½ poses.json ====================

@st.cache_data
def load_poses_data():
    """åŠ è½½é¢„è®¾å§¿åŠ¿æ•°æ®"""
    poses_file = Path(__file__).parent / "poses.json"
    try:
        with open(poses_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

POSES_DATA = load_poses_data()

# ==================== è‡ªå®šä¹‰æ ·å¼ ====================

st.markdown("""
<style>
    .main-header {
        font-size: 2.8rem;
        font-weight: bold;
        text-align: center;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f64f59 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 0.5rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .sub-header {
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .score-display {
        font-size: 4rem;
        font-weight: bold;
        text-align: center;
        padding: 1.5rem;
        border-radius: 1.5rem;
        background: linear-gradient(135deg, rgba(255,255,255,0.9), rgba(255,255,255,0.7));
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    .score-high { color: #00c853; text-shadow: 0 0 20px rgba(0,200,83,0.3); }
    .score-medium { color: #ff9800; text-shadow: 0 0 20px rgba(255,152,0,0.3); }
    .score-low { color: #f44336; text-shadow: 0 0 20px rgba(244,67,54,0.3); }
    
    .feedback-box {
        background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
        border-radius: 1rem;
        padding: 1.5rem;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
        font-size: 1.1rem;
    }
    .analyzing-box {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        border-radius: 1rem;
        padding: 2rem;
        text-align: center;
        animation: pulse 1.5s ease-in-out infinite;
    }
    @keyframes pulse {
        0%, 100% { opacity: 1; transform: scale(1); }
        50% { opacity: 0.8; transform: scale(1.02); }
    }
    .perfect-banner {
        background: linear-gradient(135deg, #00c853 0%, #64dd17 100%);
        color: white;
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        padding: 1.5rem;
        border-radius: 1rem;
        animation: celebratePulse 0.3s ease-in-out infinite alternate;
        box-shadow: 0 8px 32px rgba(0,200,83,0.4);
    }
    @keyframes celebratePulse {
        from { transform: scale(1); }
        to { transform: scale(1.05); }
    }
    .countdown {
        font-size: 8rem;
        font-weight: bold;
        text-align: center;
        color: #f64f59;
        text-shadow: 4px 4px 8px rgba(0,0,0,0.2);
        animation: countdownPop 0.5s ease-out;
    }
    @keyframes countdownPop {
        0% { transform: scale(1.5); opacity: 0; }
        100% { transform: scale(1); opacity: 1; }
    }
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 50%, #00c853 100%);
        border-radius: 10px;
    }
    .guide-step {
        background: white;
        border-radius: 1rem;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    .guide-step.active {
        border-left: 4px solid #667eea;
        background: linear-gradient(90deg, #667eea10, white);
    }
    .guide-step.completed {
        border-left: 4px solid #00c853;
        background: linear-gradient(90deg, #00c85310, white);
    }
</style>
""", unsafe_allow_html=True)

# ==================== åˆå§‹åŒ– MediaPipe ====================

# MediaPipe 0.10.x ç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†
try:
    # æ–°ç‰ˆæœ¬ MediaPipe (0.10.14+) éœ€è¦ä½¿ç”¨ legacy æ¨¡å—
    from mediapipe.python.solutions import pose as mp_pose_module
    from mediapipe.python.solutions import drawing_utils as mp_drawing
    from mediapipe.python.solutions import drawing_styles as mp_drawing_styles
    
    class MPPoseWrapper:
        """MediaPipe Pose åŒ…è£…ç±»"""
        POSE_CONNECTIONS = mp_pose_module.POSE_CONNECTIONS
        
        @staticmethod
        def Pose(**kwargs):
            return mp_pose_module.Pose(**kwargs)
    
    mp_pose = MPPoseWrapper()
    
except ImportError:
    try:
        # æ—§ç‰ˆæœ¬ MediaPipe
        import mediapipe as mp
        mp_pose = mp.solutions.pose
        mp_drawing = mp.solutions.drawing_utils
        mp_drawing_styles = mp.solutions.drawing_styles
    except AttributeError:
        # æœ€æ–°ç‰ˆæœ¬ä½¿ç”¨ç›´æ¥å¯¼å…¥
        from mediapipe.tasks.python.vision import PoseLandmarker
        from mediapipe.tasks.python.vision import PoseLandmarkerOptions
        from mediapipe.tasks.python import BaseOptions
        import cv2
        
        # ç®€åŒ–çš„ç»˜åˆ¶å‡½æ•°
        class SimpleMPPose:
            POSE_CONNECTIONS = [
                (11, 12), (11, 13), (13, 15), (12, 14), (14, 16),
                (11, 23), (12, 24), (23, 24), (23, 25), (24, 26),
                (25, 27), (26, 28)
            ]
            
            @staticmethod
            def Pose(**kwargs):
                return None
        
        mp_pose = SimpleMPPose()
        mp_drawing = None
        mp_drawing_styles = None

# ==================== Session State åˆå§‹åŒ– ====================

def init_session_state():
    defaults = {
        "match_score": 0.0,
        "high_score_start": None,
        "captured_photos": [],
        "show_perfect": False,
        "feedback": "å‡†å¤‡å¥½åç‚¹å‡»ã€Œå¼€å§‹å¼•å¯¼ã€",
        "target_pose": "standing_casual",
        "workflow_stage": "idle",  # idle -> analyzing -> guiding -> countdown -> captured
        "ai_analysis": None,
        "countdown_value": None,
        "is_analyzing": False,
        "voice_enabled": True,
        "auto_capture_threshold": 85,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

init_session_state()

# ==================== API è°ƒç”¨å‡½æ•° ====================

async def analyze_image_async(image_base64: str, user_message: str = None) -> Dict:
    """å¼‚æ­¥è°ƒç”¨åç«¯ API åˆ†æå›¾ç‰‡"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                "http://localhost:8000/analyze_image",
                json={
                    "image_base64": image_base64,
                    "user_message": user_message
                }
            )
            if response.status_code == 200:
                return response.json()
            else:
                return {"success": False, "error": f"API error: {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def get_pose_landmarks_for_drawing(pose_id: str) -> List[tuple]:
    """ä» poses.json è·å–ç”¨äºç»˜åˆ¶çš„å…³é”®ç‚¹åæ ‡"""
    if pose_id not in POSES_DATA:
        return []
    return POSES_DATA[pose_id].get("landmarks", [])


# ==================== è¯­éŸ³åé¦ˆ (æµè§ˆå™¨ç«¯ TTS) ====================

def speak_text_js(text: str):
    """ä½¿ç”¨æµè§ˆå™¨å†…ç½® TTS æ’­æ”¾è¯­éŸ³"""
    js_code = f"""
    <script>
        if ('speechSynthesis' in window) {{
            const utterance = new SpeechSynthesisUtterance("{text}");
            utterance.lang = 'zh-CN';
            utterance.rate = 1.0;
            utterance.pitch = 1.0;
            speechSynthesis.speak(utterance);
        }}
    </script>
    """
    st.components.v1.html(js_code, height=0)


# ==================== è§†é¢‘å¤„ç†ç±» ====================

class PoseVideoProcessor(VideoProcessorBase):
    """å®æ—¶è§†é¢‘å¤„ç†ï¼šéª¨éª¼æ£€æµ‹ + ç›®æ ‡å§¿åŠ¿å åŠ  + åŒ¹é…åº¦è®¡ç®—"""
    
    def __init__(self):
        self.pose = mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.current_score = 0.0
        self.current_feedback = ""
        self.frame_count = 0
        self.target_pose_id = "standing_casual"
        self.show_target_overlay = True
        self.last_frame = None
        
    def draw_target_pose_from_landmarks(self, image: np.ndarray, pose_id: str) -> np.ndarray:
        """ä» poses.json ä¸­çš„å…³é”®ç‚¹ç»˜åˆ¶ç›®æ ‡å§¿åŠ¿"""
        landmarks = get_pose_landmarks_for_drawing(pose_id)
        if not landmarks or len(landmarks) < 17:
            return image
            
        h, w = image.shape[:2]
        overlay = image.copy()
        
        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        points = [(int(lm[0] * w), int(lm[1] * h)) for lm in landmarks]
        
        # MediaPipe Pose è¿æ¥å®šä¹‰ (ä¸ŠåŠèº«å…³é”®è¿æ¥)
        connections = [
            (11, 12),  # åŒè‚©
            (11, 13), (13, 15),  # å·¦è‡‚
            (12, 14), (14, 16),  # å³è‡‚
            (11, 23), (12, 24),  # è‚©åˆ°é«‹
            (23, 24),  # åŒé«‹
        ]
        
        # ç»˜åˆ¶è™šçº¿è¿æ¥
        for start_idx, end_idx in connections:
            if start_idx < len(points) and end_idx < len(points):
                self.draw_dashed_line(overlay, points[start_idx], points[end_idx], 
                                     (0, 255, 120), 3, 15)
        
        # ç»˜åˆ¶å…³é”®ç‚¹
        key_indices = [11, 12, 13, 14, 15, 16, 23, 24]  # è‚©ã€è‚˜ã€è…•ã€é«‹
        for idx in key_indices:
            if idx < len(points):
                cv2.circle(overlay, points[idx], 15, (0, 255, 120), 2)
                cv2.circle(overlay, points[idx], 8, (0, 255, 120), -1)
        
        # æ··åˆå åŠ 
        alpha = 0.35
        return cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)
    
    def draw_dashed_line(self, img, pt1, pt2, color, thickness, dash_length):
        """ç»˜åˆ¶è™šçº¿"""
        dist = np.sqrt((pt2[0] - pt1[0])**2 + (pt2[1] - pt1[1])**2)
        if dist < 1:
            return
        dashes = max(int(dist / dash_length), 1)
        
        for i in range(0, dashes, 2):
            start_ratio = i / dashes
            end_ratio = min((i + 1) / dashes, 1.0)
            start = (int(pt1[0] + (pt2[0] - pt1[0]) * start_ratio),
                    int(pt1[1] + (pt2[1] - pt1[1]) * start_ratio))
            end = (int(pt1[0] + (pt2[0] - pt1[0]) * end_ratio),
                  int(pt1[1] + (pt2[1] - pt1[1]) * end_ratio))
            cv2.line(img, start, end, color, thickness)
    
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        """å¤„ç†æ¯ä¸€å¸§è§†é¢‘"""
        image = frame.to_ndarray(format="bgr24")
        image = cv2.flip(image, 1)  # é•œåƒ
        
        self.last_frame = image.copy()
        
        # MediaPipe å§¿æ€æ£€æµ‹
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.pose.process(rgb)
        
        # æ ¹æ®å·¥ä½œæµé˜¶æ®µå†³å®šæ˜¯å¦æ˜¾ç¤ºç›®æ ‡å§¿åŠ¿
        workflow_stage = st.session_state.get("workflow_stage", "idle")
        
        if workflow_stage in ["guiding", "countdown"] and self.show_target_overlay:
            target_pose = st.session_state.get("target_pose", self.target_pose_id)
            image = self.draw_target_pose_from_landmarks(image, target_pose)
        
        # ç»˜åˆ¶ç”¨æˆ·éª¨éª¼ + è®¡ç®—åŒ¹é…åº¦
        if results.pose_landmarks:
            # ç™½è‰²éª¨éª¼çº¿
            mp_drawing.draw_landmarks(
                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255), thickness=2, circle_radius=3),
                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255), thickness=2)
            )
            
            # è®¡ç®—åŒ¹é…åº¦ (æ¯3å¸§ä¸€æ¬¡)
            self.frame_count += 1
            if self.frame_count % 3 == 0 and workflow_stage == "guiding":
                try:
                    landmarks = [Landmark(x=lm.x, y=lm.y, z=lm.z, visibility=lm.visibility) 
                                for lm in results.pose_landmarks.landmark]
                    target_pose = st.session_state.get("target_pose", "standing_casual")
                    
                    self.current_score = calculate_pose_similarity(landmarks, target_pose)
                    feedback = get_feedback_instruction(self.current_score, landmarks, target_pose)
                    self.current_feedback = feedback or "å®Œç¾ï¼ä¿æŒä½ï¼"
                    
                    st.session_state.match_score = self.current_score
                    st.session_state.feedback = self.current_feedback
                except:
                    pass
        
        # ç»˜åˆ¶ UI è¦†ç›–å±‚
        self._draw_ui_overlay(image, workflow_stage)
        
        return av.VideoFrame.from_ndarray(image, format="bgr24")
    
    def _draw_ui_overlay(self, image, stage):
        """ç»˜åˆ¶ UI è¦†ç›–ä¿¡æ¯"""
        h, w = image.shape[:2]
        
        if stage == "analyzing":
            # åˆ†æä¸­çŠ¶æ€
            cv2.rectangle(image, (w//4, h//2-40), (3*w//4, h//2+40), (0,0,0), -1)
            cv2.rectangle(image, (w//4, h//2-40), (3*w//4, h//2+40), (255,180,0), 2)
            cv2.putText(image, "Analyzing...", (w//2-80, h//2+10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255,180,0), 2)
        
        elif stage == "guiding":
            # æ˜¾ç¤ºåˆ†æ•°
            score = st.session_state.get("match_score", 0)
            color = (0,255,0) if score >= 70 else (0,165,255) if score >= 50 else (0,0,255)
            
            cv2.rectangle(image, (10, 10), (220, 70), (0,0,0), -1)
            cv2.rectangle(image, (10, 10), (220, 70), color, 2)
            cv2.putText(image, f"Match: {score:.0f}%", (20, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2)
        
        elif stage == "countdown":
            # å€’è®¡æ—¶
            countdown = st.session_state.get("countdown_value", 3)
            if countdown:
                cv2.rectangle(image, (w//2-60, h//2-80), (w//2+60, h//2+80), (0,0,0), -1)
                cv2.putText(image, str(countdown), (w//2-30, h//2+30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 4, (0,255,255), 4)


# ==================== ä¸»åº”ç”¨ç•Œé¢ ====================

def main():
    # æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸ“¸ AI æ‹ç…§åŠ©æ‰‹</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">âœ¨ è®©æ¯ä¸€å¼ ç…§ç‰‡éƒ½å®Œç¾ Â· AI æ™ºèƒ½å¼•å¯¼æ‹æ‘„</p>', unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # é€‰æ‹©ç›®æ ‡å§¿åŠ¿
        pose_options = {k: f"{v.get('name', k)}" for k, v in POSES_DATA.items()}
        if not pose_options:
            pose_options = {"standing_casual": "è‡ªç„¶ç«™ç«‹"}
        
        selected_pose = st.selectbox(
            "ğŸ­ ç›®æ ‡å§¿åŠ¿",
            options=list(pose_options.keys()),
            format_func=lambda x: pose_options.get(x, x)
        )
        st.session_state.target_pose = selected_pose
        
        if selected_pose in POSES_DATA:
            st.info(f"ğŸ“ {POSES_DATA[selected_pose].get('description', '')}")
        
        st.divider()
        
        # è®¾ç½®
        st.session_state.auto_capture_threshold = st.slider(
            "ğŸ¯ è‡ªåŠ¨æ‹ç…§é˜ˆå€¼", 70, 100, 85
        )
        st.session_state.voice_enabled = st.checkbox("ğŸ”Š è¯­éŸ³åé¦ˆ", value=True)
        
        st.divider()
        
        # å·¥ä½œæµçŠ¶æ€
        st.subheader("ğŸ“Š å½“å‰çŠ¶æ€")
        stage_labels = {
            "idle": "â¸ï¸ å¾…å‘½",
            "analyzing": "ğŸ” åˆ†æä¸­...",
            "guiding": "ğŸ¯ å¼•å¯¼ä¸­",
            "countdown": "â±ï¸ å€’è®¡æ—¶",
            "captured": "âœ… å·²æ‹æ‘„"
        }
        st.write(stage_labels.get(st.session_state.workflow_stage, "æœªçŸ¥"))
        
        # æ‹ç…§å†å²
        st.divider()
        st.subheader("ğŸ“· æ‹ç…§å†å²")
        if st.session_state.captured_photos:
            for photo, ts in st.session_state.captured_photos[-3:]:
                st.image(photo, caption=ts, use_container_width=True)
        else:
            st.caption("è¿˜æ²¡æœ‰æ‹æ‘„ç…§ç‰‡")
    
    # ä¸»åŒºåŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“¹ å®æ—¶ç”»é¢")
        
        # WebRTC
        ctx = webrtc_streamer(
            key="pose-detection-v2",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=PoseVideoProcessor,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )
        
        # æ“ä½œæŒ‰é’®
        col_btn1, col_btn2, col_btn3 = st.columns(3)
        
        with col_btn1:
            if st.button("ğŸš€ å¼€å§‹å¼•å¯¼", use_container_width=True, type="primary",
                        disabled=st.session_state.workflow_stage != "idle"):
                start_guidance_workflow()
        
        with col_btn2:
            if st.button("ğŸ“¸ ç«‹å³æ‹ç…§", use_container_width=True,
                        disabled=st.session_state.workflow_stage not in ["guiding", "idle"]):
                trigger_capture()
        
        with col_btn3:
            if st.button("ï¿½ é‡æ–°å¼€å§‹", use_container_width=True):
                reset_workflow()
    
    with col2:
        st.subheader("ğŸ“Š åŒ¹é…çŠ¶æ€")
        
        # æ ¹æ®å·¥ä½œæµé˜¶æ®µæ˜¾ç¤ºä¸åŒå†…å®¹
        stage = st.session_state.workflow_stage
        
        if stage == "idle":
            st.markdown("""
            <div class="guide-step">
                <h4>ğŸ‘‹ å‡†å¤‡å¼€å§‹</h4>
                <p>ç‚¹å‡»ã€Œå¼€å§‹å¼•å¯¼ã€è®© AI åˆ†æåœºæ™¯å¹¶æ¨èæœ€ä½³å§¿åŠ¿</p>
            </div>
            """, unsafe_allow_html=True)
            
        elif stage == "analyzing":
            st.markdown("""
            <div class="analyzing-box">
                <h3>ğŸ” æ­£åœ¨åˆ†æç¯å¢ƒå…‰çº¿...</h3>
                <p>AI æ­£åœ¨è¯†åˆ«åœºæ™¯ç‰¹å¾</p>
            </div>
            """, unsafe_allow_html=True)
            
            # åŒæ—¶æ˜¾ç¤ºé»˜è®¤å§¿åŠ¿æç¤º
            st.info("ğŸ’¡ å…ˆä¿æŒè‡ªç„¶ç«™ç«‹å§¿åŠ¿ï¼Œç¨åä¼šæ˜¾ç¤ºæœ€ä½³å»ºè®®")
            
        elif stage == "guiding":
            # åˆ†æ•°æ˜¾ç¤º
            score = st.session_state.match_score
            score_class = "score-high" if score >= 70 else "score-medium" if score >= 50 else "score-low"
            
            st.markdown(f'<div class="score-display {score_class}">{score:.0f}%</div>', 
                       unsafe_allow_html=True)
            
            # è¿›åº¦æ¡
            st.progress(min(score / 100, 1.0))
            
            # åé¦ˆ
            st.markdown(f"""
            <div class="feedback-box">
                <strong>ğŸ’¬ AI æŒ‡å¯¼ï¼š</strong><br>
                {st.session_state.feedback}
            </div>
            """, unsafe_allow_html=True)
            
            # AI åˆ†æç»“æœ
            if st.session_state.ai_analysis:
                with st.expander("ğŸ¤– AI åœºæ™¯åˆ†æ", expanded=False):
                    analysis = st.session_state.ai_analysis
                    st.write(f"**åœºæ™¯ï¼š** {analysis.get('scene_analysis', 'N/A')}")
                    st.write(f"**å»ºè®®ï¼š** {analysis.get('composition_advice', 'N/A')}")
            
            # è‡ªåŠ¨æ‹ç…§æ£€æµ‹
            check_auto_capture()
            
        elif stage == "countdown":
            countdown = st.session_state.countdown_value
            st.markdown(f'<div class="countdown">{countdown}</div>', unsafe_allow_html=True)
            
        elif stage == "captured":
            st.markdown('<div class="perfect-banner">âœ¨ PERFECT! âœ¨</div>', unsafe_allow_html=True)
            st.balloons()
            
            if st.session_state.captured_photos:
                latest = st.session_state.captured_photos[-1]
                st.image(latest[0], caption=f"ğŸ“¸ {latest[1]}", use_container_width=True)
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### ğŸ¬ æ‹ç…§æµç¨‹
        1. **å¼€å¯æ‘„åƒå¤´** - ç‚¹å‡» START å…è®¸æ‘„åƒå¤´æƒé™
        2. **å¼€å§‹å¼•å¯¼** - AI åˆ†æåœºæ™¯å¹¶æ¨èå§¿åŠ¿
        3. **è·Ÿéšå¼•å¯¼** - ç§»åŠ¨åˆ°ç»¿è‰²è™šçº¿æ¡†ä½ç½®
        4. **è‡ªåŠ¨æ‹ç…§** - åŒ¹é…åº¦ â‰¥85% æŒç»­ 2 ç§’è‡ªåŠ¨æ‹ç…§
        
        ### ğŸ¨ å›¾ä¾‹
        - ğŸŸ¢ **ç»¿è‰²è™šçº¿** - ç›®æ ‡å§¿åŠ¿å¼•å¯¼æ¡†
        - âšª **ç™½è‰²å®çº¿** - ä½ çš„å®æ—¶éª¨éª¼
        - ğŸ“Š **è¿›åº¦æ¡** - å§¿åŠ¿åŒ¹é…ç¨‹åº¦
        """)


def start_guidance_workflow():
    """å¼€å§‹å¼•å¯¼å·¥ä½œæµ"""
    st.session_state.workflow_stage = "analyzing"
    st.session_state.is_analyzing = True
    
    # è¯­éŸ³æç¤º
    if st.session_state.voice_enabled:
        speak_text_js("æ­£åœ¨åˆ†æåœºæ™¯ï¼Œè¯·ç¨å€™")
    
    # æ¨¡æ‹Ÿ API è°ƒç”¨å»¶è¿Ÿååˆ‡æ¢åˆ°å¼•å¯¼æ¨¡å¼
    # å®é™…é¡¹ç›®ä¸­è¿™é‡Œåº”è¯¥å¼‚æ­¥è°ƒç”¨ GPT-4o
    time.sleep(0.5)  # æ¨¡æ‹Ÿ
    
    # æ¨¡æ‹Ÿ AI åˆ†æç»“æœ
    st.session_state.ai_analysis = {
        "scene_analysis": "å®¤å†…å…‰çº¿è‰¯å¥½ï¼ŒèƒŒæ™¯ç®€æ´",
        "recommended_pose_id": st.session_state.target_pose,
        "composition_advice": "ä¿æŒè‡ªç„¶ç«™å§¿ï¼Œé¢å‘é•œå¤´å¾®ç¬‘",
        "voice_feedback": "å¾ˆå¥½ï¼Œå…‰çº¿ä¸é”™ï¼Œè¯·ä¿æŒè‡ªç„¶ç«™å§¿"
    }
    
    st.session_state.workflow_stage = "guiding"
    st.session_state.is_analyzing = False
    
    if st.session_state.voice_enabled:
        speak_text_js(st.session_state.ai_analysis.get("voice_feedback", ""))
    
    st.rerun()


def check_auto_capture():
    """æ£€æŸ¥æ˜¯å¦è§¦å‘è‡ªåŠ¨æ‹ç…§"""
    score = st.session_state.match_score
    threshold = st.session_state.auto_capture_threshold
    
    if score >= threshold:
        if st.session_state.high_score_start is None:
            st.session_state.high_score_start = time.time()
        elif time.time() - st.session_state.high_score_start >= 2.0:
            trigger_countdown()
    else:
        st.session_state.high_score_start = None


def trigger_countdown():
    """è§¦å‘å€’è®¡æ—¶"""
    st.session_state.workflow_stage = "countdown"
    
    for i in [3, 2, 1]:
        st.session_state.countdown_value = i
        if st.session_state.voice_enabled:
            speak_text_js(str(i))
        time.sleep(1)
    
    trigger_capture()


def trigger_capture():
    """æ‰§è¡Œæ‹ç…§"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    
    # è¿™é‡Œåº”è¯¥ä»è§†é¢‘æµè·å–å¸§
    # ç®€åŒ–ç‰ˆæœ¬ï¼šè®°å½•æ—¶é—´æˆ³
    st.session_state.captured_photos.append((None, timestamp))
    st.session_state.workflow_stage = "captured"
    st.session_state.high_score_start = None
    
    if st.session_state.voice_enabled:
        speak_text_js("æ‹æ‘„æˆåŠŸï¼å¤ªæ£’äº†ï¼")
    
    st.rerun()


def reset_workflow():
    """é‡ç½®å·¥ä½œæµ"""
    st.session_state.workflow_stage = "idle"
    st.session_state.match_score = 0.0
    st.session_state.feedback = "å‡†å¤‡å¥½åç‚¹å‡»ã€Œå¼€å§‹å¼•å¯¼ã€"
    st.session_state.ai_analysis = None
    st.session_state.high_score_start = None
    st.session_state.countdown_value = None
    st.session_state.show_perfect = False
    st.rerun()


if __name__ == "__main__":
    main()
